{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8489ac9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "join() argument must be str, bytes, or os.PathLike object, not 'builtin_function_or_method'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26480/680007218.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m  \u001b[1;31m#       continue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0minner_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[0mcsv_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minner_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"GT-\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ananya\\lib\\ntpath.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(path, *paths)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult_drive\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mresult_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBytesWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m         \u001b[0mgenericpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_arg_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'join'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ananya\\lib\\genericpath.py\u001b[0m in \u001b[0;36m_check_arg_types\u001b[1;34m(funcname, *args)\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[0mhasbytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m             raise TypeError(f'{funcname}() argument must be str, bytes, or '\n\u001b[0m\u001b[0;32m    153\u001b[0m                             f'os.PathLike object, not {s.__class__.__name__!r}') from None\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasstr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasbytes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: join() argument must be str, bytes, or os.PathLike object, not 'builtin_function_or_method'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import imread\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "import seaborn as sns\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "### LOADING DATASET\n",
    "data_dir = os.path.abspath('~/Ananya/GTSRB/Final_Training/Images')\n",
    "os.path.exists(data_dir)\n",
    "\n",
    "### Function to resize the images using open cv\n",
    "def resize_cv(im):\n",
    "    return cv2.resize(im, (64, 64), interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "### Loading datset\n",
    "list_images = []\n",
    "output = []\n",
    "#for dir in os.listdir(data_dir):\n",
    "    #if dir == '.DS_Store' :\n",
    " #       continue\n",
    "    \n",
    "inner_dir = os.path.join(data_dir, dir)\n",
    "csv_file = pd.read_csv(os.path.join(inner_dir,\"GT-\" + dir + '.csv'), sep=';')\n",
    "for row in csv_file.iterrows() :\n",
    "    img_path = os.path.join(inner_dir, row[1].Filename)\n",
    "    img = imread(img_path)\n",
    "    img = img[row[1]['Roi.X1']:row[1]['Roi.X2'],row[1]['Roi.Y1']:row[1]['Roi.Y2'],:]\n",
    "    img = resize_cv(img)\n",
    "    list_images.append(img)\n",
    "    output.append(row[1].ClassId)\n",
    "\n",
    " \n",
    "### Plotting the dataset\n",
    "fig = sns.distplot(output, kde=False, bins = 43, hist = True, hist_kws=dict(edgecolor=\"black\", linewidth=2))\n",
    "fig.set(title = \"Traffic signs frequency graph\",\n",
    "        xlabel = \"ClassId\",\n",
    "        ylabel = \"Frequency\")\n",
    "\n",
    "input_array = np.stack(list_images)\n",
    "\n",
    "train_y = keras.utils.np_utils.to_categorical(output)\n",
    "\n",
    "### Randomizing the dataset\n",
    "randomize = np.arange(len(input_array))\n",
    "np.random.shuffle(randomize)\n",
    "x = input_array[randomize]\n",
    "y = train_y[randomize]\n",
    "\n",
    "### Splitting the dataset in train, validation, test set\n",
    "split_size = int(x.shape[0]*0.6)\n",
    "train_x, val_x = x[:split_size], x[split_size:]\n",
    "train1_y, val_y = y[:split_size], y[split_size:]\n",
    "\n",
    "split_size = int(val_x.shape[0]*0.5)\n",
    "val_x, test_x = val_x[:split_size], val_x[split_size:]\n",
    "val_y, test_y = val_y[:split_size], val_y[split_size:]\n",
    "\n",
    "\n",
    "### Building the model\n",
    "hidden_num_units = 2048\n",
    "hidden_num_units1 = 1024\n",
    "hidden_num_units2 = 128\n",
    "output_num_units = 43\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "pool_size = (2, 2)\n",
    "input_shape = Input(shape=(32, 32,3))\n",
    "\n",
    "model = Sequential([\n",
    "\n",
    " Conv2D(16, (3, 3), activation='relu', input_shape=(64,64,3), padding='same'),\n",
    " BatchNormalization(),\n",
    "\n",
    " Conv2D(16, (3, 3), activation='relu', padding='same'),\n",
    " BatchNormalization(),\n",
    " MaxPooling2D(pool_size=pool_size),\n",
    " Dropout(0.2),\n",
    "    \n",
    " Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    " BatchNormalization(),\n",
    "    \n",
    " Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    " BatchNormalization(),\n",
    " MaxPooling2D(pool_size=pool_size),\n",
    " Dropout(0.2),\n",
    "    \n",
    " Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    " BatchNormalization(),\n",
    "    \n",
    " Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    " BatchNormalization(),\n",
    " MaxPooling2D(pool_size=pool_size),\n",
    " Dropout(0.2),\n",
    "\n",
    " Flatten(),\n",
    "\n",
    " Dense(units=hidden_num_units, activation='relu'),\n",
    " Dropout(0.3),\n",
    " Dense(units=hidden_num_units1, activation='relu'),\n",
    " Dropout(0.3),\n",
    " Dense(units=hidden_num_units2, activation='relu'),\n",
    " Dropout(0.3),\n",
    " Dense(units=output_num_units, input_dim=hidden_num_units, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4), metrics=['accuracy'])\n",
    "\n",
    "### Training the model\n",
    "trained_model_conv = model.fit(train_x.reshape(-1,64,64,3), train1_y, epochs=epochs, batch_size=batch_size, validation_data=(val_x, val_y))                               \n",
    "\n",
    "### Prdicting the class\n",
    "pred = model.predict_classes(test_x)\n",
    "\n",
    "### Evaluating the model\n",
    "model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3eedab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff54c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"aa.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"Ananya\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccb67c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
